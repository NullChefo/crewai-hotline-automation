
services:
  ollama:
    image: ollama/ollama:0.1.34
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama-local:/root/.ollama
  crewai:
    image: crewai-agents
    container_name: crewai-agents
    depends_on:
      - ollama
    extra_hosts:
      - "telemetry.crewai.com:127.0.0.1" # To avoid 'Connection to telemetry.crewai.com timed out' error when using local LLM
volumes:
  ollama-local:
    external: true